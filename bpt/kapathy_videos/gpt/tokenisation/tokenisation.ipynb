{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"\"\"\n",
    "Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training\n",
    "on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic\n",
    "in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of\n",
    "thousands of examples. By contrast, humans can generally perform a new language task from only\n",
    "a few examples or from simple instructions – something which current NLP systems still largely\n",
    "struggle to do. Here we show that scaling up language models greatly improves task-agnostic,\n",
    "few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-\n",
    "tuning approaches.\n",
    "\"\"\"\n",
    "s2 = s1+ \"\"\"\n",
    "\n",
    "Specifically, we train GPT-3, an autoregressive language model with 175 billion\n",
    "parameters, 10x more than any previous non-sparse language model, and test its performance in\n",
    "the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning,\n",
    "with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3\n",
    "achieves strong performance on many NLP datasets, including translation, question-answering, and\n",
    "cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as\n",
    "unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same\n",
    "time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some\n",
    "datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally,\n",
    "we find that GPT-3 can generate samples of news articles which human evaluators have difficulty\n",
    "distinguishing from article.\n",
    "\n",
    "Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly\n",
    "flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word\n",
    "vectors [MCCD13, PSM14] and fed to task-specific architectures, then RNNs with multiple layers of representations\n",
    "and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to\n",
    "task-specific architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have\n",
    "been directly fine-tuned, entirely removing the need for task-specific architectures [RNSS18, DCLT18, HR18].\n",
    "This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension,\n",
    "question answering, textual entailment, and many others, and has continued to advance based on new architectures\n",
    "and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while\n",
    "the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve\n",
    "strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands\n",
    "of examples specific to that task. Removing this limitation would be desirable, for several reasons.\n",
    "First, from a practical perspective, the need for a large dataset of labeled examples for every new task limits the\n",
    "applicability of language models. There exists a very wide range of possible useful language tasks, encompassing\n",
    "anything from correcting grammar, to generating examples of an abstract concept, to critiquing a short story. For many\n",
    "of these tasks it is difficult to collect a large supervised training dataset, especially when the process must be repeated\n",
    "for every new task.\n",
    "Second, the potential to exploit spurious correlations in training data fundamentally grows with the expressiveness\n",
    "of the model and the narrowness of the training distribution. This can create problems for the pre-training plus\n",
    "fine-tuning paradigm, where models are designed to be large to absorb information during pre-training, but are then\n",
    "fine-tuned on very narrow task distributions. For instance [HLW+20] observe that larger models do not necessarily\n",
    "generalize better out-of-distribution. There is evidence that suggests that the generalization achieved under this paradigm\n",
    "can be poor because the model is overly specific to the training distribution and does not generalize well outside it\n",
    "[YdC+19, MPL19]. Thus, the performance of fine-tuned models on specific benchmarks, even when it is nominally at\n",
    "human-level, may exaggerate actual performance on the underlying task [GSL+18, NK19].\n",
    "Third, humans do not require large supervised datasets to learn most language tasks – a brief directive in natural\n",
    "language (e.g. “please tell me if this sentence describes something happy or something sad”) or at most a tiny number\n",
    "of demonstrations (e.g. “here are two examples of people acting brave; please give a third example of bravery”) is often\n",
    "Figure 1.1: Language model meta-learning. During unsupervised pre-training, a language model develops a broad\n",
    "set of skills and pattern recognition abilities. It then uses these abilities at inference time to rapidly adapt to or recognize\n",
    "the desired task. We use the term “in-context learning” to describe the inner loop of this process, which occurs within\n",
    "the forward-pass upon each sequence. The sequences in this diagram are not intended to be representative of the data a\n",
    "model would see during pre-training, but are intended to show that there are sometimes repeated sub-tasks embedded\n",
    "within a single sequence.\n",
    "3\n",
    "Figure 1.2: Larger models make increasingly efficient use of in-context information. We show in-context learning\n",
    "performance on a simple task requiring the model to remove random symbols from a word, both with and without a\n",
    "natural language task description (see Sec. 3.9.2). The steeper “in-context learning curves” for large models demonstrate\n",
    "improved ability to learn a task from contextual information. We see qualitatively similar behavior across a wide range\n",
    "of tasks.\n",
    "sufficient to enable a human to perform a new task to at least a reasonable degree of competence. Aside from pointing\n",
    "to a conceptual limitation in our current NLP techniques, this adaptability has practical advantages – it allows humans\n",
    "to seamlessly mix together or switch between many tasks and skills, for example performing addition during a lengthy\n",
    "dialogue. To be broadly useful, we would someday like our NLP systems to have this same fluidity and generality.\n",
    "One potential route towards addressing these issues is meta-learning1 – which in the context of language models means\n",
    "the model develops a broad set of skills and pattern recognition abilities at training time, and then uses those abilities\n",
    "at inference time to rapidly adapt to or recognize the desired task (illustrated in Figure 1.1). Recent work [RWC+19]\n",
    "attempts to do this via what we call “in-context learning”, using the text input of a pretrained language model as a form\n",
    "of task specification: the model is conditioned on a natural language instruction and/or a few demonstrations of the task\n",
    "and is then expected to complete further instances of the task simply by predicting what comes next.\n",
    "While it has shown some initial promise, this approach still achieves results far inferior to fine-tuning – for example\n",
    "[RWC+19] achieves only 4% on Natural Questions, and even its 55 F1 CoQa result is now more than 35 points behind\n",
    "the state of the art. Meta-learning clearly requires substantial improvement in order to be viable as a practical method of\n",
    "solving language tasks.\n",
    "Another recent trend in language modeling may offer a way forward. In recent years the capacity of transformer\n",
    "language models has increased substantially, from 100 million parameters [RNSS18], to 300 million parameters\n",
    "[DCLT18], to 1.5 billion parameters [RWC+19], to 8 billion parameters [SPP+19], 11 billion parameters [RSR+19],\n",
    "and finally 17 billion parameters [Tur20]. Each increase has brought improvements in text synthesis and/or downstream\n",
    "NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a\n",
    "smooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and\n",
    "tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong\n",
    "gains with scale.\n",
    "1In the context of language models this has sometimes been called “zero-shot transfer”, but this term is potentially ambiguous:\n",
    "the method is “zero-shot” in the sense that no gradient updates are performed, but it often involves providing inference-time\n",
    "demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term “meta-learning”\n",
    "to capture the inner-loop / outer-loop structure of the general method, and the term “in context-learning” to refer to the inner\n",
    "loop of meta-learning. We further specialize the description to “zero-shot”, “one-shot”, or “few-shot” depending on how many\n",
    "demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model\n",
    "learns new tasks from scratch at inference time or simply recognizes patterns seen during training – this is an important issue which\n",
    "we discuss later in the paper, but “meta-learning” is intended to encompass both possibilities, and simply describes the inner-outer\n",
    "loop structure.\n",
    "4\n",
    "Figure 1.3: Aggregate performance for all 42 accuracy-denominated benchmarks While zero-shot performance\n",
    "improves steadily with model size, few-shot performance increases more rapidly, demonstrating that larger models are\n",
    "more proficient at in-context learning. See Figure 3.8 for a more detailed analysis on SuperGLUE, a standard NLP\n",
    "benchmark suite.\n",
    "In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call\n",
    "GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets,\n",
    "as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training\n",
    "set. For each task, we evaluate GPT-3 under 3 conditions: (a) “few-shot learning”, or in-context learning where we\n",
    "allow as many demonstrations as will fit into the model’s context window (typically 10 to 100), (b) “one-shot learning”,\n",
    "where we allow only one demonstration, and (c) “zero-shot” learning, where no demonstrations are allowed and only\n",
    "an instruction in natural language is given to the model. GPT-3 could also in principle be evaluated in the traditional\n",
    "fine-tuning setting, but we leave this to future work.\n",
    "Figure 1.2 illustrates the conditions we study, and shows few-shot learning of a simple task requiring the model to\n",
    "remove extraneous symbols from a word. Model performance improves with the addition of a natural language task\n",
    "description, and with the number of examples in the model’s context, K. Few-shot learning also improves dramatically\n",
    "with model size. Though the results in this case are particularly striking, the general trends with both model size and\n",
    "number of examples in-context hold for most tasks we study. We emphasize that these “learning” curves involve no\n",
    "gradient updates or fine-tuning, just increasing numbers of demonstrations given as conditioning.\n",
    "Broadly, on NLP tasks GPT-3 achieves promising results in the zero-shot and one-shot settings, and in the the few-shot\n",
    "setting is sometimes competitive with or even occasionally surpasses state-of-the-art (despite state-of-the-art being held\n",
    "by fine-tuned models). For example, GPT-3 achieves 81.5 F1 on CoQA in the zero-shot setting, 84.0 F1 on CoQA in\n",
    "the one-shot setting, 85.0 F1 in the few-shot setting. Similarly, GPT-3 achieves 64.3% accuracy on TriviaQA in the\n",
    "zero-shot setting, 68.0% in the one-shot setting, and 71.2% in the few-shot setting, the last of which is state-of-the-art\n",
    "relative to fine-tuned models operating in the same closed-book setting.\n",
    "GPT-3 also displays one-shot and few-shot proficiency at tasks designed to test rapid adaption or on-the-fly reasoning,\n",
    "which include unscrambling words, performing arithmetic, and using novel words in a sentence after seeing them\n",
    "defined only once. We also show that in the few-shot setting, GPT-3 can generate synthetic news articles which human\n",
    "evaluators have difficulty distinguishing from human-generated articles.\n",
    "At the same time, we also find some tasks on which few-shot performance struggles, even at the scale of GPT-3. This\n",
    "includes natural language inference tasks like the ANLI dataset, and some reading comprehension datasets like RACE\n",
    "or QuAC. By presenting a broad characterization of GPT-3’s strengths and weaknesses, including these limitations, we\n",
    "hope to stimulate study of few-shot learning in language models and draw attention to where progress is most needed.\n",
    "A heuristic sense of the overall results can be seen in Figure 1.3, which aggregates the various tasks (though it should\n",
    "not be seen as a rigorous or meaningful benchmark in itself).\n",
    "5\n",
    "We also undertake a systematic study of “data contamination” – a growing problem when training high capacity models\n",
    "on datasets such as Common Crawl, which can potentially include content from test datasets simply because such\n",
    "content often exists on the web. In this paper we develop systematic tools to measure data contamination and quantify\n",
    "its distorting effects. Although we find that data contamination has a minimal effect on GPT-3’s performance on most\n",
    "datasets, we do identify a few datasets where it could be inflating results, and we either do not report results on these\n",
    "datasets or we note them with an asterisk, depending on the severity.\n",
    "In addition to all the above, we also train a series of smaller models (ranging from 125 million parameters to 13 billion\n",
    "parameters) in order to compare their performance to GPT-3 in the zero, one and few-shot settings. Broadly, for most\n",
    "tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap\n",
    "between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models\n",
    "are more proficient meta-learners.\n",
    "Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and\n",
    "broader societal impacts, and attempt a preliminary analysis of GPT-3’s characteristics in this regard.\n",
    "The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training\n",
    "GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings.\n",
    "Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3.\n",
    "Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[200], line 92\u001b[0m\n\u001b[1;32m     89\u001b[0m gpt2_regex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md| ?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m+| ?\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m+| ?[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{L}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;132;01m{N}\u001b[39;00m\u001b[38;5;124m]+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+(?!\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS)|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     91\u001b[0m t \u001b[38;5;241m=\u001b[39m BytePairEncodingTokenizer(gpt2_regex)\n\u001b[0;32m---> 92\u001b[0m ids \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m encoded \u001b[38;5;241m=\u001b[39m t\u001b[38;5;241m.\u001b[39mencode(s1)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCompression ration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(s1)\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mlen\u001b[39m(encoded))\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[200], line 34\u001b[0m, in \u001b[0;36mBytePairEncodingTokenizer.train\u001b[0;34m(self, training_string, desired_vocab_length)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_merges):\n\u001b[1;32m     33\u001b[0m     stats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_stats(ids)\n\u001b[0;32m---> 34\u001b[0m     top_pair \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstats\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge(ids, top_pair, replacement\u001b[38;5;241m=\u001b[39mnew_token)\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerges[top_pair] \u001b[38;5;241m=\u001b[39m new_token\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "from typing import Counter\n",
    "from collections import deque\n",
    "\n",
    "class BytePairEncodingTokenizer:\n",
    "    def __init__(self) -> None:\n",
    "        self.stats = Counter()\n",
    "        self.desired_vocab_size = 700\n",
    "        self.merges: dict[tuple[int,int], int] = {}\n",
    "        self.vocab = {}\n",
    "        \n",
    "    def get_stats(self, tokens: list[int]) -> dict[tuple[int, int], int]:\n",
    "        stats = Counter()\n",
    "        for pair in zip(tokens, tokens[1:]):\n",
    "            stats[pair] += 1\n",
    "            \n",
    "        return stats\n",
    "        \n",
    "    def train(self, training_string: str, desired_vocab_length: int = 4096):\n",
    "        enc = training_string.encode('utf-8')\n",
    "        tokens = list(map(int, enc))\n",
    "\n",
    "        vocab_size = len(self.get_stats(tokens))\n",
    "        num_merges = vocab_size - desired_vocab_length\n",
    "        ids = list(tokens)\n",
    "        new_token = 256\n",
    "        for _ in range(num_merges):\n",
    "            stats = self.get_stats(ids)\n",
    "            top_pair = max(stats, key=stats.get)\n",
    "            ids = self.merge(ids, top_pair, replacement=new_token)\n",
    "            self.merges[top_pair] = new_token\n",
    "            new_token+=1\n",
    "            \n",
    "        for pair, token in self.merges.items():\n",
    "            self.vocab[token] = pair\n",
    "        \n",
    "        return ids\n",
    "        \n",
    "    # Will achieve less compression than Kapathy's algo but runs in O(n) instead of O(n^2)\n",
    "    def encode(self, s: str) -> list[int]:\n",
    "        tokens = list(map(int, s.encode('utf-8')))\n",
    "        final_idx = len(tokens)-2\n",
    "        idx = 0\n",
    "        while idx < final_idx:\n",
    "            pair = (tokens[idx], tokens[idx+1])\n",
    "            if pair in self.merges:\n",
    "                tokens[idx] = self.merges[pair]\n",
    "                del tokens[idx+1]\n",
    "                final_idx-=1\n",
    "            \n",
    "            idx+=1\n",
    "        return tokens\n",
    "        \n",
    "        \n",
    "    def merge(self, input_ids: list[int], pair: tuple[int, int], replacement: int) -> list[int]:\n",
    "        final_index = len(input_ids)\n",
    "        idx = 0\n",
    "        result = []\n",
    "        while idx < final_index:\n",
    "            if idx + 1 < final_index and input_ids[idx] == pair[0] and input_ids[idx+1] == pair[1]:\n",
    "                result.append(replacement)\n",
    "                idx+=2\n",
    "            else:\n",
    "                result.append(input_ids[idx])\n",
    "                idx+=1\n",
    "                \n",
    "        return result\n",
    "    \n",
    "    def decode(self,tokens: list[int]) -> str:\n",
    "        if len(tokens) == 0:\n",
    "            return ''\n",
    "        result: list[int] = []\n",
    "        for token in tokens:\n",
    "            queue = deque([token])\n",
    "            while len(queue) > 0:\n",
    "                current = queue.popleft()\n",
    "                if current < 256:\n",
    "                    result.append(current)\n",
    "                else:\n",
    "                    new_pair = self.vocab[current]\n",
    "                    queue.appendleft(new_pair[1])\n",
    "                    queue.appendleft(new_pair[0])\n",
    "        return bytes(result).decode('utf-8', errors='replace')\n",
    "                    \n",
    "                \n",
    "        \n",
    "            \n",
    "t = BytePairEncodingTokenizer()\n",
    "ids = t.train(s2, 600)\n",
    "encoded = t.encode(s1)\n",
    "print(f'Compression ration: {(len(s1)/ len(encoded)):.2f}x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "421"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(t.vocab.keys())\n",
    "# len(t.vocab) + 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hey',\n",
       " ' beech',\n",
       " ' how',\n",
       " ' are',\n",
       " ' you',\n",
       " '.',\n",
       " ' you',\n",
       " \"'ll\",\n",
       " ' hate',\n",
       " ' this',\n",
       " '.',\n",
       " ' 15',\n",
       " '+',\n",
       " '1993',\n",
       " '=',\n",
       " '22',\n",
       " 'dumby']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2pat = re.compile(r)\n",
    "\n",
    "gpt2pat.findall(\"Hey beech how are you. you'll hate this. 15+1993=22dumby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79,\n",
       " 307,\n",
       " 32,\n",
       " 98,\n",
       " 273,\n",
       " 105,\n",
       " 99,\n",
       " 32,\n",
       " 112,\n",
       " 269,\n",
       " 45,\n",
       " 116,\n",
       " 288,\n",
       " 257,\n",
       " 257,\n",
       " 103,\n",
       " 261,\n",
       " 112,\n",
       " 112,\n",
       " 293,\n",
       " 97,\n",
       " 294,\n",
       " 276,\n",
       " 257,\n",
       " 99,\n",
       " 108,\n",
       " 117,\n",
       " 308,\n",
       " 110,\n",
       " 103,\n",
       " 32,\n",
       " 290,\n",
       " 274,\n",
       " 108,\n",
       " 276,\n",
       " 100,\n",
       " 277,\n",
       " 97,\n",
       " 276,\n",
       " 268,\n",
       " 275,\n",
       " 116,\n",
       " 288,\n",
       " 257,\n",
       " 257,\n",
       " 103,\n",
       " 276,\n",
       " 105,\n",
       " 258,\n",
       " 115,\n",
       " 105,\n",
       " 109,\n",
       " 105,\n",
       " 108,\n",
       " 272,\n",
       " 259,\n",
       " 286,\n",
       " 271,\n",
       " 256,\n",
       " 112,\n",
       " 293,\n",
       " 99,\n",
       " 311,\n",
       " 258,\n",
       " 274,\n",
       " 115,\n",
       " 99,\n",
       " 114,\n",
       " 105,\n",
       " 98,\n",
       " 101,\n",
       " 275,\n",
       " 257,\n",
       " 32,\n",
       " 91,\n",
       " 82,\n",
       " 87,\n",
       " 67,\n",
       " 43,\n",
       " 49,\n",
       " 57,\n",
       " 93,\n",
       " 44,\n",
       " 10,\n",
       " 119,\n",
       " 315,\n",
       " 104,\n",
       " 32,\n",
       " 269,\n",
       " 108,\n",
       " 277,\n",
       " 105,\n",
       " 118,\n",
       " 101,\n",
       " 108,\n",
       " 281,\n",
       " 279,\n",
       " 288,\n",
       " 105,\n",
       " 103,\n",
       " 104,\n",
       " 116,\n",
       " 102,\n",
       " 267,\n",
       " 119,\n",
       " 272,\n",
       " 275,\n",
       " 115,\n",
       " 99,\n",
       " 278,\n",
       " 257,\n",
       " 103,\n",
       " 32,\n",
       " 117,\n",
       " 112,\n",
       " 32,\n",
       " 280,\n",
       " 259,\n",
       " 104,\n",
       " 256,\n",
       " 290,\n",
       " 274,\n",
       " 108,\n",
       " 301,\n",
       " 105,\n",
       " 122,\n",
       " 101,\n",
       " 276,\n",
       " 100,\n",
       " 277,\n",
       " 273,\n",
       " 292,\n",
       " 301,\n",
       " 105,\n",
       " 122,\n",
       " 256,\n",
       " 268,\n",
       " 275,\n",
       " 308,\n",
       " 118,\n",
       " 262,\n",
       " 115,\n",
       " 315,\n",
       " 121,\n",
       " 276,\n",
       " 268,\n",
       " 275,\n",
       " 282,\n",
       " 110,\n",
       " 103,\n",
       " 271,\n",
       " 32,\n",
       " 280,\n",
       " 259,\n",
       " 288,\n",
       " 257,\n",
       " 257,\n",
       " 103,\n",
       " 289,\n",
       " 79,\n",
       " 307,\n",
       " 32,\n",
       " 117,\n",
       " 115,\n",
       " 101,\n",
       " 10,\n",
       " 280,\n",
       " 32,\n",
       " 257,\n",
       " 45,\n",
       " 99,\n",
       " 260,\n",
       " 116,\n",
       " 300,\n",
       " 266,\n",
       " 282,\n",
       " 272,\n",
       " 110,\n",
       " 257,\n",
       " 103,\n",
       " 32,\n",
       " 105,\n",
       " 258,\n",
       " 278,\n",
       " 115,\n",
       " 286,\n",
       " 115,\n",
       " 105,\n",
       " 109,\n",
       " 105,\n",
       " 108,\n",
       " 272,\n",
       " 259,\n",
       " 286,\n",
       " 91,\n",
       " 82,\n",
       " 87,\n",
       " 67,\n",
       " 43,\n",
       " 49,\n",
       " 57,\n",
       " 93,\n",
       " 276,\n",
       " 98,\n",
       " 117,\n",
       " 266,\n",
       " 257,\n",
       " 259,\n",
       " 104,\n",
       " 105,\n",
       " 258,\n",
       " 119,\n",
       " 267,\n",
       " 107,\n",
       " 32,\n",
       " 119,\n",
       " 256,\n",
       " 115,\n",
       " 121,\n",
       " 279,\n",
       " 101,\n",
       " 109,\n",
       " 277,\n",
       " 105,\n",
       " 99,\n",
       " 278,\n",
       " 108,\n",
       " 281,\n",
       " 300,\n",
       " 112,\n",
       " 108,\n",
       " 267,\n",
       " 256,\n",
       " 308,\n",
       " 102,\n",
       " 102,\n",
       " 262,\n",
       " 265,\n",
       " 266,\n",
       " 115,\n",
       " 292,\n",
       " 264,\n",
       " 110,\n",
       " 103,\n",
       " 258,\n",
       " 102,\n",
       " 267,\n",
       " 10,\n",
       " 282,\n",
       " 272,\n",
       " 110,\n",
       " 257,\n",
       " 103,\n",
       " 32,\n",
       " 119,\n",
       " 315,\n",
       " 104,\n",
       " 257,\n",
       " 259,\n",
       " 104,\n",
       " 256,\n",
       " 99,\n",
       " 260,\n",
       " 116,\n",
       " 300,\n",
       " 116,\n",
       " 289,\n",
       " 84,\n",
       " 104,\n",
       " 262,\n",
       " 101,\n",
       " 102,\n",
       " 267,\n",
       " 101,\n",
       " 276,\n",
       " 119,\n",
       " 256,\n",
       " 279,\n",
       " 272,\n",
       " 266,\n",
       " 271,\n",
       " 105,\n",
       " 258,\n",
       " 115,\n",
       " 305,\n",
       " 264,\n",
       " 260,\n",
       " 32,\n",
       " 98,\n",
       " 281,\n",
       " 300,\n",
       " 112,\n",
       " 108,\n",
       " 105,\n",
       " 99,\n",
       " 315,\n",
       " 108,\n",
       " 281,\n",
       " 274,\n",
       " 102,\n",
       " 257,\n",
       " 257,\n",
       " 103,\n",
       " 261,\n",
       " 110,\n",
       " 275,\n",
       " 99,\n",
       " 260,\n",
       " 116,\n",
       " 288,\n",
       " 279,\n",
       " 257,\n",
       " 103,\n",
       " 259,\n",
       " 104,\n",
       " 256,\n",
       " 308,\n",
       " 102,\n",
       " 102,\n",
       " 262,\n",
       " 265,\n",
       " 266,\n",
       " 115,\n",
       " 292,\n",
       " 264,\n",
       " 110,\n",
       " 103,\n",
       " 115,\n",
       " 10,\n",
       " 271,\n",
       " 277,\n",
       " 32,\n",
       " 119,\n",
       " 256,\n",
       " 119,\n",
       " 105,\n",
       " 309,\n",
       " 32,\n",
       " 98,\n",
       " 256,\n",
       " 101,\n",
       " 118,\n",
       " 278,\n",
       " 117,\n",
       " 277,\n",
       " 257,\n",
       " 103,\n",
       " 32,\n",
       " 71,\n",
       " 80,\n",
       " 84,\n",
       " 45,\n",
       " 51,\n",
       " 32,\n",
       " 260,\n",
       " 32,\n",
       " 267,\n",
       " 32,\n",
       " 99,\n",
       " 111,\n",
       " 117,\n",
       " 108,\n",
       " 275,\n",
       " 257,\n",
       " 32,\n",
       " 112,\n",
       " 114,\n",
       " 257,\n",
       " 99,\n",
       " 105,\n",
       " 112,\n",
       " 282,\n",
       " 32,\n",
       " 101,\n",
       " 118,\n",
       " 278,\n",
       " 117,\n",
       " 277,\n",
       " 256,\n",
       " 71,\n",
       " 80,\n",
       " 84,\n",
       " 45,\n",
       " 51,\n",
       " 32,\n",
       " 260,\n",
       " 289,\n",
       " 84,\n",
       " 104,\n",
       " 311,\n",
       " 256,\n",
       " 115,\n",
       " 292,\n",
       " 264,\n",
       " 110,\n",
       " 103,\n",
       " 258,\n",
       " 99,\n",
       " 268,\n",
       " 32,\n",
       " 98,\n",
       " 256,\n",
       " 115,\n",
       " 101,\n",
       " 265,\n",
       " 261,\n",
       " 258,\n",
       " 108,\n",
       " 121,\n",
       " 257,\n",
       " 103,\n",
       " 32,\n",
       " 260,\n",
       " 261,\n",
       " 10,\n",
       " 115,\n",
       " 112,\n",
       " 305,\n",
       " 116,\n",
       " 114,\n",
       " 117,\n",
       " 109,\n",
       " 32,\n",
       " 280,\n",
       " 32,\n",
       " 298,\n",
       " 119,\n",
       " 32,\n",
       " 109,\n",
       " 117,\n",
       " 294,\n",
       " 259,\n",
       " 273,\n",
       " 107,\n",
       " 45,\n",
       " 115,\n",
       " 112,\n",
       " 305,\n",
       " 105,\n",
       " 102,\n",
       " 105,\n",
       " 99,\n",
       " 32,\n",
       " 100,\n",
       " 277,\n",
       " 97,\n",
       " 259,\n",
       " 104,\n",
       " 101,\n",
       " 281,\n",
       " 116,\n",
       " 265,\n",
       " 275,\n",
       " 116,\n",
       " 286,\n",
       " 269,\n",
       " 108,\n",
       " 281,\n",
       " 260,\n",
       " 289,\n",
       " 83,\n",
       " 112,\n",
       " 305,\n",
       " 105,\n",
       " 102,\n",
       " 105,\n",
       " 99,\n",
       " 278,\n",
       " 108,\n",
       " 121,\n",
       " 276,\n",
       " 119,\n",
       " 256,\n",
       " 99,\n",
       " 268,\n",
       " 32,\n",
       " 105,\n",
       " 274,\n",
       " 110,\n",
       " 264,\n",
       " 102,\n",
       " 281,\n",
       " 277,\n",
       " 32,\n",
       " 282,\n",
       " 273,\n",
       " 266,\n",
       " 102,\n",
       " 111,\n",
       " 307,\n",
       " 32,\n",
       " 112,\n",
       " 111,\n",
       " 257,\n",
       " 116,\n",
       " 258,\n",
       " 260,\n",
       " 259,\n",
       " 104,\n",
       " 105,\n",
       " 115,\n",
       " 10,\n",
       " 115,\n",
       " 112,\n",
       " 305,\n",
       " 116,\n",
       " 114,\n",
       " 117,\n",
       " 109,\n",
       " 32,\n",
       " 40,\n",
       " 115,\n",
       " 101,\n",
       " 256,\n",
       " 70,\n",
       " 105,\n",
       " 103,\n",
       " 307,\n",
       " 256,\n",
       " 50,\n",
       " 46,\n",
       " 49,\n",
       " 32,\n",
       " 102,\n",
       " 267,\n",
       " 261,\n",
       " 110,\n",
       " 32,\n",
       " 105,\n",
       " 309,\n",
       " 117,\n",
       " 279,\n",
       " 288,\n",
       " 264,\n",
       " 260,\n",
       " 41]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.encode(\"\"\"Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19],\n",
    "with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use\n",
    "of in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for\n",
    "learning within the context. Therefore, we start this section by explicitly defining and contrasting the different settings\n",
    "that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a\n",
    "spectrum of how much task-specific data they tend to rely on. Specifically, we can identify at least four points on this\n",
    "spectrum (see Figure 2.1 for an illustration)\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
