{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_size=32033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "dataset_size = len(words)\n",
    "print(f'{dataset_size=}')\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocabulary of characters and mapping to/from integers\n",
    "special_character = '.'\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s: i+1 for i,s in enumerate(chars)} # string to int\n",
    "stoi[special_character] = 0\n",
    "itos = {i: s for s, i in stoi.items()} # int to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape=torch.Size([182625, 3]), y.shape=torch.Size([182625])\n",
      "X.shape=torch.Size([22655, 3]), y.shape=torch.Size([22655])\n",
      "X.shape=torch.Size([22866, 3]), y.shape=torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "# build dataset\n",
    "def build_dataset(words: list[str], block_size: int = 3):\n",
    "    X, y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size\n",
    "        for ch in w + special_character:\n",
    "            idx = stoi[ch]\n",
    "            X.append(context)\n",
    "            y.append(idx)\n",
    "            context = context[1:] + [idx]\n",
    "    \n",
    "    X, y = torch.tensor(X), torch.tensor(y)\n",
    "    print(f'{X.shape=}, {y.shape=}')\n",
    "    return X, y\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(dataset_size*.8)\n",
    "n2 = int(dataset_size*.9)\n",
    "X_train, y_train = build_dataset(words[:n1])\n",
    "X_dev, y_dev = build_dataset(words[n1:n2])\n",
    "X_test, y_test = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter_count=17697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(29.5216)"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implementing the hidden layer\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "V = len(stoi.items())\n",
    "neurons = 300\n",
    "embedding_size = 10\n",
    "block_size = 3\n",
    "sample_size = X_train.shape[0]\n",
    "\n",
    "# initial random embeddings\n",
    "C = torch.randn((V,embedding_size))\n",
    "\n",
    "# look up the embeddings for the training data\n",
    "emb = C[X_train]\n",
    "\n",
    "# initialise the learnable parameters of the first layer of neurons\n",
    "W1 = torch.randn((block_size*embedding_size, neurons), generator=g)\n",
    "b1 = torch.randn(neurons, generator=g)\n",
    "\n",
    "# flatten the embeddings to concatenate the vector for each input character giving us a n x (embedding size x block size) matrix\n",
    "emb_reshaped = emb.view((sample_size, block_size*embedding_size))\n",
    "\n",
    "# apply activation function\n",
    "h = torch.tanh(emb_reshaped @ W1 + b1)\n",
    "\n",
    "# start implementing the output layer by randomly initializing the learnable parameters\n",
    "W2 = torch.randn((neurons, V), generator=g)\n",
    "b2 = torch.randn(V, generator=g)\n",
    "logits = h @ W2 + b2\n",
    "# Manual implementation of cross entropy that's less efficient and doesn't regularise to prevent explosions towards infinity for high logit values\n",
    "# counts = logits.exp()\n",
    "# counts.shape\n",
    "# prob = counts /counts.sum(1, keepdim=True)\n",
    "loss = F.cross_entropy(logits, y_train)\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "parameter_count = sum([p.nelement() for p in parameters])\n",
    "print(f'{parameter_count=}')\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "# initialise tracking datasets\n",
    "lru = []\n",
    "losses = []\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, loss = 2.3337550163269043\n",
      "iteration: 10, loss = 2.282813549041748\n",
      "iteration: 20, loss = 2.2773189544677734\n",
      "iteration: 30, loss = 2.275055408477783\n",
      "iteration: 40, loss = 2.26460337638855\n",
      "iteration: 50, loss = 2.2716212272644043\n",
      "iteration: 60, loss = 2.273630380630493\n",
      "iteration: 70, loss = 2.268428087234497\n",
      "iteration: 80, loss = 2.2807369232177734\n",
      "iteration: 90, loss = 2.28108286857605\n",
      "iteration: 100, loss = 2.272984266281128\n",
      "iteration: 110, loss = 2.260239362716675\n",
      "iteration: 120, loss = 2.2818644046783447\n",
      "iteration: 130, loss = 2.2772140502929688\n",
      "iteration: 140, loss = 2.2717490196228027\n",
      "iteration: 150, loss = 2.2839276790618896\n",
      "iteration: 160, loss = 2.270928382873535\n",
      "iteration: 170, loss = 2.2665140628814697\n",
      "iteration: 180, loss = 2.2756974697113037\n",
      "iteration: 190, loss = 2.2599120140075684\n",
      "iteration: 200, loss = 2.2745578289031982\n",
      "iteration: 210, loss = 2.2623424530029297\n",
      "iteration: 220, loss = 2.254526138305664\n",
      "iteration: 230, loss = 2.2623696327209473\n",
      "iteration: 240, loss = 2.2708311080932617\n",
      "iteration: 250, loss = 2.2801246643066406\n",
      "iteration: 260, loss = 2.2750496864318848\n",
      "iteration: 270, loss = 2.2613892555236816\n",
      "iteration: 280, loss = 2.26120924949646\n",
      "iteration: 290, loss = 2.272064208984375\n",
      "iteration: 300, loss = 2.2728946208953857\n",
      "iteration: 310, loss = 2.2661890983581543\n",
      "iteration: 320, loss = 2.2728989124298096\n",
      "iteration: 330, loss = 2.2778570652008057\n",
      "iteration: 340, loss = 2.275273084640503\n",
      "iteration: 350, loss = 2.2753937244415283\n",
      "iteration: 360, loss = 2.281709671020508\n",
      "iteration: 370, loss = 2.2622885704040527\n",
      "iteration: 380, loss = 2.268683910369873\n",
      "iteration: 390, loss = 2.2724313735961914\n",
      "iteration: 400, loss = 2.2563889026641846\n",
      "iteration: 410, loss = 2.266955852508545\n",
      "iteration: 420, loss = 2.2580809593200684\n",
      "iteration: 430, loss = 2.2659528255462646\n",
      "iteration: 440, loss = 2.265636682510376\n",
      "iteration: 450, loss = 2.2736287117004395\n",
      "iteration: 460, loss = 2.2769784927368164\n",
      "iteration: 470, loss = 2.28053879737854\n",
      "iteration: 480, loss = 2.276756763458252\n",
      "iteration: 490, loss = 2.2713146209716797\n",
      "iteration: 500, loss = 2.2652218341827393\n",
      "iteration: 510, loss = 2.264549732208252\n",
      "iteration: 520, loss = 2.244654893875122\n",
      "iteration: 530, loss = 2.258695602416992\n",
      "iteration: 540, loss = 2.265611410140991\n",
      "iteration: 550, loss = 2.2689898014068604\n",
      "iteration: 560, loss = 2.25728178024292\n",
      "iteration: 570, loss = 2.2785539627075195\n",
      "iteration: 580, loss = 2.269561767578125\n",
      "iteration: 590, loss = 2.259371757507324\n",
      "iteration: 600, loss = 2.289104700088501\n",
      "iteration: 610, loss = 2.2628798484802246\n",
      "iteration: 620, loss = 2.272282361984253\n",
      "iteration: 630, loss = 2.251194953918457\n",
      "iteration: 640, loss = 2.2718746662139893\n",
      "iteration: 650, loss = 2.2701823711395264\n",
      "iteration: 660, loss = 2.2591042518615723\n",
      "iteration: 670, loss = 2.2583892345428467\n",
      "iteration: 680, loss = 2.262624502182007\n",
      "iteration: 690, loss = 2.258368492126465\n",
      "iteration: 700, loss = 2.2689919471740723\n",
      "iteration: 710, loss = 2.2647602558135986\n",
      "iteration: 720, loss = 2.272925615310669\n",
      "iteration: 730, loss = 2.26767635345459\n",
      "iteration: 740, loss = 2.2733004093170166\n",
      "iteration: 750, loss = 2.259946823120117\n",
      "iteration: 760, loss = 2.2701609134674072\n",
      "iteration: 770, loss = 2.2569124698638916\n",
      "iteration: 780, loss = 2.267911434173584\n",
      "iteration: 790, loss = 2.2638792991638184\n",
      "iteration: 800, loss = 2.2636284828186035\n",
      "iteration: 810, loss = 2.2710330486297607\n",
      "iteration: 820, loss = 2.261046886444092\n",
      "iteration: 830, loss = 2.266514539718628\n",
      "iteration: 840, loss = 2.264160633087158\n",
      "iteration: 850, loss = 2.2685928344726562\n",
      "iteration: 860, loss = 2.2615606784820557\n",
      "iteration: 870, loss = 2.2535698413848877\n",
      "iteration: 880, loss = 2.2756803035736084\n",
      "iteration: 890, loss = 2.267932653427124\n",
      "iteration: 900, loss = 2.2617347240448\n",
      "iteration: 910, loss = 2.263303756713867\n",
      "iteration: 920, loss = 2.2688560485839844\n",
      "iteration: 930, loss = 2.2571985721588135\n",
      "iteration: 940, loss = 2.2709696292877197\n",
      "iteration: 950, loss = 2.2719168663024902\n",
      "iteration: 960, loss = 2.2706921100616455\n",
      "iteration: 970, loss = 2.266949415206909\n",
      "iteration: 980, loss = 2.280846118927002\n",
      "iteration: 990, loss = 2.2569997310638428\n",
      "iteration: 1000, loss = 2.265636682510376\n",
      "iteration: 1010, loss = 2.252516508102417\n",
      "iteration: 1020, loss = 2.261237859725952\n",
      "iteration: 1030, loss = 2.279505729675293\n",
      "iteration: 1040, loss = 2.2674248218536377\n",
      "iteration: 1050, loss = 2.274113416671753\n",
      "iteration: 1060, loss = 2.2664568424224854\n",
      "iteration: 1070, loss = 2.2678706645965576\n",
      "iteration: 1080, loss = 2.2739877700805664\n",
      "iteration: 1090, loss = 2.274117946624756\n",
      "iteration: 1100, loss = 2.2536792755126953\n",
      "iteration: 1110, loss = 2.2609567642211914\n",
      "iteration: 1120, loss = 2.2669312953948975\n",
      "iteration: 1130, loss = 2.2601351737976074\n",
      "iteration: 1140, loss = 2.263110399246216\n",
      "iteration: 1150, loss = 2.2602946758270264\n",
      "iteration: 1160, loss = 2.2632522583007812\n",
      "iteration: 1170, loss = 2.27917218208313\n",
      "iteration: 1180, loss = 2.2656893730163574\n",
      "iteration: 1190, loss = 2.2576122283935547\n",
      "iteration: 1200, loss = 2.26347279548645\n",
      "iteration: 1210, loss = 2.266247510910034\n",
      "iteration: 1220, loss = 2.272839069366455\n",
      "iteration: 1230, loss = 2.2633180618286133\n",
      "iteration: 1240, loss = 2.261366128921509\n",
      "iteration: 1250, loss = 2.27724552154541\n",
      "iteration: 1260, loss = 2.274529218673706\n",
      "iteration: 1270, loss = 2.265913963317871\n",
      "iteration: 1280, loss = 2.258021354675293\n",
      "iteration: 1290, loss = 2.2647597789764404\n",
      "iteration: 1300, loss = 2.2726805210113525\n",
      "iteration: 1310, loss = 2.266221761703491\n",
      "iteration: 1320, loss = 2.2552871704101562\n",
      "iteration: 1330, loss = 2.2784669399261475\n",
      "iteration: 1340, loss = 2.263411521911621\n",
      "iteration: 1350, loss = 2.2656803131103516\n",
      "iteration: 1360, loss = 2.2659552097320557\n",
      "iteration: 1370, loss = 2.267620801925659\n",
      "iteration: 1380, loss = 2.276336193084717\n",
      "iteration: 1390, loss = 2.263129472732544\n",
      "iteration: 1400, loss = 2.274829387664795\n",
      "iteration: 1410, loss = 2.266125202178955\n",
      "iteration: 1420, loss = 2.2647907733917236\n",
      "iteration: 1430, loss = 2.267441987991333\n",
      "iteration: 1440, loss = 2.2635786533355713\n",
      "iteration: 1450, loss = 2.260714292526245\n",
      "iteration: 1460, loss = 2.2657508850097656\n",
      "iteration: 1470, loss = 2.2748804092407227\n",
      "iteration: 1480, loss = 2.2659194469451904\n",
      "iteration: 1490, loss = 2.2618956565856934\n",
      "iteration: 1500, loss = 2.2664880752563477\n",
      "iteration: 1510, loss = 2.2545151710510254\n",
      "iteration: 1520, loss = 2.2619874477386475\n",
      "iteration: 1530, loss = 2.2630558013916016\n",
      "iteration: 1540, loss = 2.2749924659729004\n",
      "iteration: 1550, loss = 2.267371654510498\n",
      "iteration: 1560, loss = 2.2658424377441406\n",
      "iteration: 1570, loss = 2.245302677154541\n",
      "iteration: 1580, loss = 2.2757468223571777\n",
      "iteration: 1590, loss = 2.2666819095611572\n",
      "iteration: 1600, loss = 2.2589399814605713\n",
      "iteration: 1610, loss = 2.264202117919922\n",
      "iteration: 1620, loss = 2.2588207721710205\n",
      "iteration: 1630, loss = 2.2675929069519043\n",
      "iteration: 1640, loss = 2.266335964202881\n",
      "iteration: 1650, loss = 2.2726149559020996\n",
      "iteration: 1660, loss = 2.2583601474761963\n",
      "iteration: 1670, loss = 2.2558395862579346\n",
      "iteration: 1680, loss = 2.277515411376953\n",
      "iteration: 1690, loss = 2.271793842315674\n",
      "iteration: 1700, loss = 2.2640159130096436\n",
      "iteration: 1710, loss = 2.2688448429107666\n",
      "iteration: 1720, loss = 2.267836809158325\n",
      "iteration: 1730, loss = 2.274914264678955\n",
      "iteration: 1740, loss = 2.2680954933166504\n",
      "iteration: 1750, loss = 2.275611639022827\n",
      "iteration: 1760, loss = 2.265207529067993\n",
      "iteration: 1770, loss = 2.270144462585449\n",
      "iteration: 1780, loss = 2.277312755584717\n",
      "iteration: 1790, loss = 2.2655742168426514\n",
      "iteration: 1800, loss = 2.271804094314575\n",
      "iteration: 1810, loss = 2.2622647285461426\n",
      "iteration: 1820, loss = 2.272616147994995\n",
      "iteration: 1830, loss = 2.269395351409912\n",
      "iteration: 1840, loss = 2.2657413482666016\n",
      "iteration: 1850, loss = 2.2715373039245605\n",
      "iteration: 1860, loss = 2.274829149246216\n",
      "iteration: 1870, loss = 2.270019292831421\n",
      "iteration: 1880, loss = 2.2742738723754883\n",
      "iteration: 1890, loss = 2.2588136196136475\n",
      "iteration: 1900, loss = 2.28255033493042\n",
      "iteration: 1910, loss = 2.263683319091797\n",
      "iteration: 1920, loss = 2.260598659515381\n",
      "iteration: 1930, loss = 2.2499475479125977\n",
      "iteration: 1940, loss = 2.2722246646881104\n",
      "iteration: 1950, loss = 2.2706003189086914\n",
      "iteration: 1960, loss = 2.267518997192383\n",
      "iteration: 1970, loss = 2.269303321838379\n",
      "iteration: 1980, loss = 2.2683615684509277\n",
      "iteration: 1990, loss = 2.274764060974121\n",
      "iteration: 2000, loss = 2.267064094543457\n",
      "iteration: 2010, loss = 2.2524282932281494\n",
      "iteration: 2020, loss = 2.2728919982910156\n",
      "iteration: 2030, loss = 2.271658182144165\n",
      "iteration: 2040, loss = 2.251809597015381\n",
      "iteration: 2050, loss = 2.268265962600708\n",
      "iteration: 2060, loss = 2.2587368488311768\n",
      "iteration: 2070, loss = 2.267989158630371\n",
      "iteration: 2080, loss = 2.2601051330566406\n",
      "iteration: 2090, loss = 2.2663626670837402\n",
      "iteration: 2100, loss = 2.2536964416503906\n",
      "iteration: 2110, loss = 2.2734601497650146\n",
      "iteration: 2120, loss = 2.2758193016052246\n",
      "iteration: 2130, loss = 2.2684385776519775\n",
      "iteration: 2140, loss = 2.2643511295318604\n",
      "iteration: 2150, loss = 2.2661893367767334\n",
      "iteration: 2160, loss = 2.2539021968841553\n",
      "iteration: 2170, loss = 2.281592845916748\n",
      "iteration: 2180, loss = 2.2671000957489014\n",
      "iteration: 2190, loss = 2.2646284103393555\n",
      "iteration: 2200, loss = 2.275750160217285\n",
      "iteration: 2210, loss = 2.2655928134918213\n",
      "iteration: 2220, loss = 2.262406349182129\n",
      "iteration: 2230, loss = 2.2606589794158936\n",
      "iteration: 2240, loss = 2.271787405014038\n",
      "iteration: 2250, loss = 2.2649970054626465\n",
      "iteration: 2260, loss = 2.258836269378662\n",
      "iteration: 2270, loss = 2.25126314163208\n",
      "iteration: 2280, loss = 2.258617639541626\n",
      "iteration: 2290, loss = 2.2645230293273926\n",
      "iteration: 2300, loss = 2.2604005336761475\n",
      "iteration: 2310, loss = 2.2649447917938232\n",
      "iteration: 2320, loss = 2.2660953998565674\n",
      "iteration: 2330, loss = 2.2596466541290283\n",
      "iteration: 2340, loss = 2.2593743801116943\n",
      "iteration: 2350, loss = 2.2630136013031006\n",
      "iteration: 2360, loss = 2.268003225326538\n",
      "iteration: 2370, loss = 2.2712483406066895\n",
      "iteration: 2380, loss = 2.2595086097717285\n",
      "iteration: 2390, loss = 2.2647130489349365\n",
      "iteration: 2400, loss = 2.2757906913757324\n",
      "iteration: 2410, loss = 2.263247489929199\n",
      "iteration: 2420, loss = 2.2620885372161865\n",
      "iteration: 2430, loss = 2.2624502182006836\n",
      "iteration: 2440, loss = 2.2726194858551025\n",
      "iteration: 2450, loss = 2.2657618522644043\n",
      "iteration: 2460, loss = 2.2764968872070312\n",
      "iteration: 2470, loss = 2.2593939304351807\n",
      "iteration: 2480, loss = 2.2705953121185303\n",
      "iteration: 2490, loss = 2.264535903930664\n",
      "iteration: 2500, loss = 2.2627460956573486\n",
      "iteration: 2510, loss = 2.2586305141448975\n",
      "iteration: 2520, loss = 2.2679803371429443\n",
      "iteration: 2530, loss = 2.2609589099884033\n",
      "iteration: 2540, loss = 2.2631986141204834\n",
      "iteration: 2550, loss = 2.2633121013641357\n",
      "iteration: 2560, loss = 2.269188404083252\n",
      "iteration: 2570, loss = 2.2740275859832764\n",
      "iteration: 2580, loss = 2.2689950466156006\n",
      "iteration: 2590, loss = 2.260152816772461\n",
      "iteration: 2600, loss = 2.2651751041412354\n",
      "iteration: 2610, loss = 2.259852886199951\n",
      "iteration: 2620, loss = 2.2569851875305176\n",
      "iteration: 2630, loss = 2.2700343132019043\n",
      "iteration: 2640, loss = 2.2582411766052246\n",
      "iteration: 2650, loss = 2.271460771560669\n",
      "iteration: 2660, loss = 2.2666895389556885\n",
      "iteration: 2670, loss = 2.262044668197632\n",
      "iteration: 2680, loss = 2.2636377811431885\n",
      "iteration: 2690, loss = 2.2672715187072754\n",
      "iteration: 2700, loss = 2.269055128097534\n",
      "iteration: 2710, loss = 2.2636749744415283\n",
      "iteration: 2720, loss = 2.263715982437134\n",
      "iteration: 2730, loss = 2.268812894821167\n",
      "iteration: 2740, loss = 2.2619152069091797\n",
      "iteration: 2750, loss = 2.2557239532470703\n",
      "iteration: 2760, loss = 2.2754054069519043\n",
      "iteration: 2770, loss = 2.2597484588623047\n",
      "iteration: 2780, loss = 2.2547829151153564\n",
      "iteration: 2790, loss = 2.2702996730804443\n",
      "iteration: 2800, loss = 2.2671921253204346\n",
      "iteration: 2810, loss = 2.2727725505828857\n",
      "iteration: 2820, loss = 2.258476734161377\n",
      "iteration: 2830, loss = 2.266024112701416\n",
      "iteration: 2840, loss = 2.275804042816162\n",
      "iteration: 2850, loss = 2.255584239959717\n",
      "iteration: 2860, loss = 2.2689335346221924\n",
      "iteration: 2870, loss = 2.2680070400238037\n",
      "iteration: 2880, loss = 2.269882917404175\n",
      "iteration: 2890, loss = 2.2681615352630615\n",
      "iteration: 2900, loss = 2.2596750259399414\n",
      "iteration: 2910, loss = 2.2685725688934326\n",
      "iteration: 2920, loss = 2.2676026821136475\n",
      "iteration: 2930, loss = 2.263465404510498\n",
      "iteration: 2940, loss = 2.251811981201172\n",
      "iteration: 2950, loss = 2.2647006511688232\n",
      "iteration: 2960, loss = 2.2669475078582764\n",
      "iteration: 2970, loss = 2.2716469764709473\n",
      "iteration: 2980, loss = 2.2526297569274902\n",
      "iteration: 2990, loss = 2.2697746753692627\n",
      "iteration: 3000, loss = 2.2634224891662598\n",
      "iteration: 3010, loss = 2.2666561603546143\n",
      "iteration: 3020, loss = 2.267033576965332\n",
      "iteration: 3030, loss = 2.2532448768615723\n",
      "iteration: 3040, loss = 2.2593259811401367\n",
      "iteration: 3050, loss = 2.257143020629883\n",
      "iteration: 3060, loss = 2.275773763656616\n",
      "iteration: 3070, loss = 2.2620394229888916\n",
      "iteration: 3080, loss = 2.264704704284668\n",
      "iteration: 3090, loss = 2.2708566188812256\n",
      "iteration: 3100, loss = 2.2676143646240234\n",
      "iteration: 3110, loss = 2.258852005004883\n",
      "iteration: 3120, loss = 2.2692618370056152\n",
      "iteration: 3130, loss = 2.281359910964966\n",
      "iteration: 3140, loss = 2.249746799468994\n",
      "iteration: 3150, loss = 2.2629661560058594\n",
      "iteration: 3160, loss = 2.2615714073181152\n",
      "iteration: 3170, loss = 2.2549402713775635\n",
      "iteration: 3180, loss = 2.258620262145996\n",
      "iteration: 3190, loss = 2.263981819152832\n",
      "iteration: 3200, loss = 2.265573263168335\n",
      "iteration: 3210, loss = 2.2767252922058105\n",
      "iteration: 3220, loss = 2.273084878921509\n",
      "iteration: 3230, loss = 2.2703568935394287\n",
      "iteration: 3240, loss = 2.270559549331665\n",
      "iteration: 3250, loss = 2.2582719326019287\n",
      "iteration: 3260, loss = 2.2735109329223633\n",
      "iteration: 3270, loss = 2.2670845985412598\n",
      "iteration: 3280, loss = 2.2628328800201416\n",
      "iteration: 3290, loss = 2.2653706073760986\n",
      "iteration: 3300, loss = 2.2701618671417236\n",
      "iteration: 3310, loss = 2.267427682876587\n",
      "iteration: 3320, loss = 2.2712602615356445\n",
      "iteration: 3330, loss = 2.265826940536499\n",
      "iteration: 3340, loss = 2.271493911743164\n",
      "iteration: 3350, loss = 2.262071132659912\n",
      "iteration: 3360, loss = 2.271028757095337\n",
      "iteration: 3370, loss = 2.2644050121307373\n",
      "iteration: 3380, loss = 2.2663400173187256\n",
      "iteration: 3390, loss = 2.261040210723877\n",
      "iteration: 3400, loss = 2.2592124938964844\n",
      "iteration: 3410, loss = 2.2672760486602783\n",
      "iteration: 3420, loss = 2.2565934658050537\n",
      "iteration: 3430, loss = 2.2496416568756104\n",
      "iteration: 3440, loss = 2.2598936557769775\n",
      "iteration: 3450, loss = 2.2544965744018555\n",
      "iteration: 3460, loss = 2.2607157230377197\n",
      "iteration: 3470, loss = 2.2559635639190674\n",
      "iteration: 3480, loss = 2.274430990219116\n",
      "iteration: 3490, loss = 2.2570929527282715\n",
      "iteration: 3500, loss = 2.2647860050201416\n",
      "iteration: 3510, loss = 2.261939525604248\n",
      "iteration: 3520, loss = 2.267096519470215\n",
      "iteration: 3530, loss = 2.259571075439453\n",
      "iteration: 3540, loss = 2.272387742996216\n",
      "iteration: 3550, loss = 2.259866237640381\n",
      "iteration: 3560, loss = 2.27215838432312\n",
      "iteration: 3570, loss = 2.277815580368042\n",
      "iteration: 3580, loss = 2.2618963718414307\n",
      "iteration: 3590, loss = 2.257391929626465\n",
      "iteration: 3600, loss = 2.2515594959259033\n",
      "iteration: 3610, loss = 2.258897066116333\n",
      "iteration: 3620, loss = 2.2679200172424316\n",
      "iteration: 3630, loss = 2.2609057426452637\n",
      "iteration: 3640, loss = 2.2676784992218018\n",
      "iteration: 3650, loss = 2.27162504196167\n",
      "iteration: 3660, loss = 2.2663016319274902\n",
      "iteration: 3670, loss = 2.264369249343872\n",
      "iteration: 3680, loss = 2.263803005218506\n",
      "iteration: 3690, loss = 2.267859935760498\n",
      "iteration: 3700, loss = 2.2739007472991943\n",
      "iteration: 3710, loss = 2.2627615928649902\n",
      "iteration: 3720, loss = 2.269092082977295\n",
      "iteration: 3730, loss = 2.259532928466797\n",
      "iteration: 3740, loss = 2.262664318084717\n",
      "iteration: 3750, loss = 2.2703239917755127\n",
      "iteration: 3760, loss = 2.2575924396514893\n",
      "iteration: 3770, loss = 2.2567338943481445\n",
      "iteration: 3780, loss = 2.256568431854248\n",
      "iteration: 3790, loss = 2.2599241733551025\n",
      "iteration: 3800, loss = 2.2673141956329346\n",
      "iteration: 3810, loss = 2.2626841068267822\n",
      "iteration: 3820, loss = 2.2659993171691895\n",
      "iteration: 3830, loss = 2.265352249145508\n",
      "iteration: 3840, loss = 2.265075206756592\n",
      "iteration: 3850, loss = 2.2615175247192383\n",
      "iteration: 3860, loss = 2.2603588104248047\n",
      "iteration: 3870, loss = 2.2684590816497803\n",
      "iteration: 3880, loss = 2.2689459323883057\n",
      "iteration: 3890, loss = 2.255976915359497\n",
      "iteration: 3900, loss = 2.261117696762085\n",
      "iteration: 3910, loss = 2.2685277462005615\n",
      "iteration: 3920, loss = 2.2659194469451904\n",
      "iteration: 3930, loss = 2.274275302886963\n",
      "iteration: 3940, loss = 2.2754735946655273\n",
      "iteration: 3950, loss = 2.266862154006958\n",
      "iteration: 3960, loss = 2.262407064437866\n",
      "iteration: 3970, loss = 2.267828941345215\n",
      "iteration: 3980, loss = 2.28310227394104\n",
      "iteration: 3990, loss = 2.2630369663238525\n",
      "iteration: 4000, loss = 2.2719876766204834\n",
      "iteration: 4010, loss = 2.2687180042266846\n",
      "iteration: 4020, loss = 2.2696962356567383\n",
      "iteration: 4030, loss = 2.2743515968322754\n",
      "iteration: 4040, loss = 2.255159378051758\n",
      "iteration: 4050, loss = 2.2557435035705566\n",
      "iteration: 4060, loss = 2.2686853408813477\n",
      "iteration: 4070, loss = 2.270918369293213\n",
      "iteration: 4080, loss = 2.2884976863861084\n",
      "iteration: 4090, loss = 2.262089252471924\n",
      "iteration: 4100, loss = 2.2706568241119385\n",
      "iteration: 4110, loss = 2.269127607345581\n",
      "iteration: 4120, loss = 2.27307391166687\n",
      "iteration: 4130, loss = 2.2568318843841553\n",
      "iteration: 4140, loss = 2.2637064456939697\n",
      "iteration: 4150, loss = 2.264873743057251\n",
      "iteration: 4160, loss = 2.266756057739258\n",
      "iteration: 4170, loss = 2.2761449813842773\n",
      "iteration: 4180, loss = 2.262970447540283\n",
      "iteration: 4190, loss = 2.2720272541046143\n",
      "iteration: 4200, loss = 2.257232666015625\n",
      "iteration: 4210, loss = 2.2604689598083496\n",
      "iteration: 4220, loss = 2.2639036178588867\n",
      "iteration: 4230, loss = 2.256664276123047\n",
      "iteration: 4240, loss = 2.2539594173431396\n",
      "iteration: 4250, loss = 2.2598788738250732\n",
      "iteration: 4260, loss = 2.2583372592926025\n",
      "iteration: 4270, loss = 2.256784200668335\n",
      "iteration: 4280, loss = 2.258822441101074\n",
      "iteration: 4290, loss = 2.258049726486206\n",
      "iteration: 4300, loss = 2.257246255874634\n",
      "iteration: 4310, loss = 2.260572671890259\n",
      "iteration: 4320, loss = 2.2566111087799072\n",
      "iteration: 4330, loss = 2.258222818374634\n",
      "iteration: 4340, loss = 2.2511425018310547\n",
      "iteration: 4350, loss = 2.277076482772827\n",
      "iteration: 4360, loss = 2.2711238861083984\n",
      "iteration: 4370, loss = 2.267082691192627\n",
      "iteration: 4380, loss = 2.265615224838257\n",
      "iteration: 4390, loss = 2.258917808532715\n",
      "iteration: 4400, loss = 2.2572989463806152\n",
      "iteration: 4410, loss = 2.2454421520233154\n",
      "iteration: 4420, loss = 2.273402690887451\n",
      "iteration: 4430, loss = 2.259105682373047\n",
      "iteration: 4440, loss = 2.267815589904785\n",
      "iteration: 4450, loss = 2.255166530609131\n",
      "iteration: 4460, loss = 2.2694592475891113\n",
      "iteration: 4470, loss = 2.250122308731079\n",
      "iteration: 4480, loss = 2.2691245079040527\n",
      "iteration: 4490, loss = 2.260680913925171\n",
      "iteration: 4500, loss = 2.258232355117798\n",
      "iteration: 4510, loss = 2.2560641765594482\n",
      "iteration: 4520, loss = 2.273098945617676\n",
      "iteration: 4530, loss = 2.2651138305664062\n",
      "iteration: 4540, loss = 2.2657485008239746\n",
      "iteration: 4550, loss = 2.2502992153167725\n",
      "iteration: 4560, loss = 2.26278018951416\n",
      "iteration: 4570, loss = 2.2788491249084473\n",
      "iteration: 4580, loss = 2.262716770172119\n",
      "iteration: 4590, loss = 2.26116681098938\n",
      "iteration: 4600, loss = 2.276923179626465\n",
      "iteration: 4610, loss = 2.275214672088623\n",
      "iteration: 4620, loss = 2.2616348266601562\n",
      "iteration: 4630, loss = 2.261709690093994\n",
      "iteration: 4640, loss = 2.2571616172790527\n",
      "iteration: 4650, loss = 2.265354871749878\n",
      "iteration: 4660, loss = 2.2469561100006104\n",
      "iteration: 4670, loss = 2.2638649940490723\n",
      "iteration: 4680, loss = 2.259125232696533\n",
      "iteration: 4690, loss = 2.2613935470581055\n",
      "iteration: 4700, loss = 2.261828899383545\n",
      "iteration: 4710, loss = 2.264518976211548\n",
      "iteration: 4720, loss = 2.2658512592315674\n",
      "iteration: 4730, loss = 2.260267496109009\n",
      "iteration: 4740, loss = 2.2610580921173096\n",
      "iteration: 4750, loss = 2.2730226516723633\n",
      "iteration: 4760, loss = 2.2611162662506104\n",
      "iteration: 4770, loss = 2.2654149532318115\n",
      "iteration: 4780, loss = 2.259824752807617\n",
      "iteration: 4790, loss = 2.262587547302246\n",
      "iteration: 4800, loss = 2.2585465908050537\n",
      "iteration: 4810, loss = 2.256690263748169\n",
      "iteration: 4820, loss = 2.255687713623047\n",
      "iteration: 4830, loss = 2.2544875144958496\n",
      "iteration: 4840, loss = 2.265615701675415\n",
      "iteration: 4850, loss = 2.265925645828247\n",
      "iteration: 4860, loss = 2.2723288536071777\n",
      "iteration: 4870, loss = 2.271786689758301\n",
      "iteration: 4880, loss = 2.268289566040039\n",
      "iteration: 4890, loss = 2.25947904586792\n",
      "iteration: 4900, loss = 2.2634317874908447\n",
      "iteration: 4910, loss = 2.2598965167999268\n",
      "iteration: 4920, loss = 2.265948534011841\n",
      "iteration: 4930, loss = 2.2575266361236572\n",
      "iteration: 4940, loss = 2.263240098953247\n",
      "iteration: 4950, loss = 2.2681689262390137\n",
      "iteration: 4960, loss = 2.259782314300537\n",
      "iteration: 4970, loss = 2.259993314743042\n",
      "iteration: 4980, loss = 2.2642576694488525\n",
      "iteration: 4990, loss = 2.25788950920105\n",
      "iteration: 5000, loss = 2.271859884262085\n",
      "iteration: 5010, loss = 2.2618982791900635\n",
      "iteration: 5020, loss = 2.262402296066284\n",
      "iteration: 5030, loss = 2.2656211853027344\n",
      "iteration: 5040, loss = 2.261483907699585\n",
      "iteration: 5050, loss = 2.272951126098633\n",
      "iteration: 5060, loss = 2.275075674057007\n",
      "iteration: 5070, loss = 2.2716782093048096\n",
      "iteration: 5080, loss = 2.2663326263427734\n",
      "iteration: 5090, loss = 2.258251905441284\n",
      "iteration: 5100, loss = 2.2609784603118896\n",
      "iteration: 5110, loss = 2.263892889022827\n",
      "iteration: 5120, loss = 2.262814998626709\n",
      "iteration: 5130, loss = 2.257549524307251\n",
      "iteration: 5140, loss = 2.265000581741333\n",
      "iteration: 5150, loss = 2.2551567554473877\n",
      "iteration: 5160, loss = 2.2474029064178467\n",
      "iteration: 5170, loss = 2.2534186840057373\n",
      "iteration: 5180, loss = 2.272019624710083\n",
      "iteration: 5190, loss = 2.274549961090088\n",
      "iteration: 5200, loss = 2.2616684436798096\n",
      "iteration: 5210, loss = 2.2616257667541504\n",
      "iteration: 5220, loss = 2.2575862407684326\n",
      "iteration: 5230, loss = 2.2566254138946533\n",
      "iteration: 5240, loss = 2.2626137733459473\n",
      "iteration: 5250, loss = 2.2682204246520996\n",
      "iteration: 5260, loss = 2.2497341632843018\n",
      "iteration: 5270, loss = 2.2625443935394287\n",
      "iteration: 5280, loss = 2.265596628189087\n",
      "iteration: 5290, loss = 2.2681639194488525\n",
      "iteration: 5300, loss = 2.2622735500335693\n",
      "iteration: 5310, loss = 2.2713229656219482\n",
      "iteration: 5320, loss = 2.2652645111083984\n",
      "iteration: 5330, loss = 2.270872116088867\n",
      "iteration: 5340, loss = 2.265787363052368\n",
      "iteration: 5350, loss = 2.2678794860839844\n",
      "iteration: 5360, loss = 2.264003276824951\n",
      "iteration: 5370, loss = 2.2564637660980225\n",
      "iteration: 5380, loss = 2.253283739089966\n",
      "iteration: 5390, loss = 2.2650113105773926\n",
      "iteration: 5400, loss = 2.2602171897888184\n",
      "iteration: 5410, loss = 2.2599575519561768\n",
      "iteration: 5420, loss = 2.261756420135498\n",
      "iteration: 5430, loss = 2.2723071575164795\n",
      "iteration: 5440, loss = 2.2735350131988525\n",
      "iteration: 5450, loss = 2.262554883956909\n",
      "iteration: 5460, loss = 2.25524640083313\n",
      "iteration: 5470, loss = 2.257108449935913\n",
      "iteration: 5480, loss = 2.254047393798828\n",
      "iteration: 5490, loss = 2.25628662109375\n",
      "iteration: 5500, loss = 2.255715847015381\n",
      "iteration: 5510, loss = 2.2701990604400635\n",
      "iteration: 5520, loss = 2.272517442703247\n",
      "iteration: 5530, loss = 2.2745795249938965\n",
      "iteration: 5540, loss = 2.263010025024414\n",
      "iteration: 5550, loss = 2.2663395404815674\n",
      "iteration: 5560, loss = 2.268143653869629\n",
      "iteration: 5570, loss = 2.268446445465088\n",
      "iteration: 5580, loss = 2.262096405029297\n",
      "iteration: 5590, loss = 2.259042263031006\n",
      "iteration: 5600, loss = 2.2725584506988525\n",
      "iteration: 5610, loss = 2.273122787475586\n",
      "iteration: 5620, loss = 2.2595252990722656\n",
      "iteration: 5630, loss = 2.2653446197509766\n",
      "iteration: 5640, loss = 2.2691309452056885\n",
      "iteration: 5650, loss = 2.2481679916381836\n",
      "iteration: 5660, loss = 2.267691135406494\n",
      "iteration: 5670, loss = 2.251218557357788\n",
      "iteration: 5680, loss = 2.2632737159729004\n",
      "iteration: 5690, loss = 2.253859519958496\n",
      "iteration: 5700, loss = 2.2690694332122803\n",
      "iteration: 5710, loss = 2.260836362838745\n",
      "iteration: 5720, loss = 2.265488862991333\n",
      "iteration: 5730, loss = 2.2667760848999023\n",
      "iteration: 5740, loss = 2.2582006454467773\n",
      "iteration: 5750, loss = 2.268170118331909\n",
      "iteration: 5760, loss = 2.2718069553375244\n",
      "iteration: 5770, loss = 2.263430118560791\n",
      "iteration: 5780, loss = 2.2580971717834473\n",
      "iteration: 5790, loss = 2.254054069519043\n",
      "iteration: 5800, loss = 2.2654783725738525\n",
      "iteration: 5810, loss = 2.268901824951172\n",
      "iteration: 5820, loss = 2.2635109424591064\n",
      "iteration: 5830, loss = 2.2673583030700684\n",
      "iteration: 5840, loss = 2.2694196701049805\n",
      "iteration: 5850, loss = 2.265751600265503\n",
      "iteration: 5860, loss = 2.2607343196868896\n",
      "iteration: 5870, loss = 2.250624895095825\n",
      "iteration: 5880, loss = 2.2618048191070557\n",
      "iteration: 5890, loss = 2.261669874191284\n",
      "iteration: 5900, loss = 2.2564406394958496\n",
      "iteration: 5910, loss = 2.279916524887085\n",
      "iteration: 5920, loss = 2.2547080516815186\n",
      "iteration: 5930, loss = 2.262531280517578\n",
      "iteration: 5940, loss = 2.256648063659668\n",
      "iteration: 5950, loss = 2.2622628211975098\n",
      "iteration: 5960, loss = 2.251492738723755\n",
      "iteration: 5970, loss = 2.266117572784424\n",
      "iteration: 5980, loss = 2.2676639556884766\n",
      "iteration: 5990, loss = 2.273414134979248\n",
      "iteration: 6000, loss = 2.2557897567749023\n",
      "iteration: 6010, loss = 2.2688522338867188\n",
      "iteration: 6020, loss = 2.2642037868499756\n",
      "iteration: 6030, loss = 2.2510814666748047\n",
      "iteration: 6040, loss = 2.263054609298706\n",
      "iteration: 6050, loss = 2.2562685012817383\n",
      "iteration: 6060, loss = 2.265197277069092\n",
      "iteration: 6070, loss = 2.269087076187134\n",
      "iteration: 6080, loss = 2.273212194442749\n",
      "iteration: 6090, loss = 2.2600135803222656\n",
      "iteration: 6100, loss = 2.2565720081329346\n",
      "iteration: 6110, loss = 2.25881028175354\n",
      "iteration: 6120, loss = 2.258049964904785\n",
      "iteration: 6130, loss = 2.275994062423706\n",
      "iteration: 6140, loss = 2.2488341331481934\n",
      "iteration: 6150, loss = 2.270341157913208\n",
      "iteration: 6160, loss = 2.263381242752075\n",
      "iteration: 6170, loss = 2.267214775085449\n",
      "iteration: 6180, loss = 2.2626547813415527\n",
      "iteration: 6190, loss = 2.249119758605957\n",
      "iteration: 6200, loss = 2.257297992706299\n",
      "iteration: 6210, loss = 2.2607297897338867\n",
      "iteration: 6220, loss = 2.266623020172119\n",
      "iteration: 6230, loss = 2.2598183155059814\n",
      "iteration: 6240, loss = 2.2705461978912354\n",
      "iteration: 6250, loss = 2.2605643272399902\n",
      "iteration: 6260, loss = 2.2643401622772217\n",
      "iteration: 6270, loss = 2.263664722442627\n",
      "iteration: 6280, loss = 2.265115976333618\n",
      "iteration: 6290, loss = 2.2644741535186768\n",
      "iteration: 6300, loss = 2.2447969913482666\n",
      "iteration: 6310, loss = 2.2541399002075195\n",
      "iteration: 6320, loss = 2.264599323272705\n",
      "iteration: 6330, loss = 2.256012201309204\n",
      "iteration: 6340, loss = 2.2676947116851807\n",
      "iteration: 6350, loss = 2.2655301094055176\n",
      "iteration: 6360, loss = 2.258270502090454\n",
      "iteration: 6370, loss = 2.253746271133423\n",
      "iteration: 6380, loss = 2.2720677852630615\n",
      "iteration: 6390, loss = 2.2593014240264893\n",
      "iteration: 6400, loss = 2.2661945819854736\n",
      "iteration: 6410, loss = 2.2501332759857178\n",
      "iteration: 6420, loss = 2.2608847618103027\n",
      "iteration: 6430, loss = 2.25571608543396\n",
      "iteration: 6440, loss = 2.2546846866607666\n",
      "iteration: 6450, loss = 2.274066209793091\n",
      "iteration: 6460, loss = 2.2645933628082275\n",
      "iteration: 6470, loss = 2.2649831771850586\n",
      "iteration: 6480, loss = 2.248420238494873\n",
      "iteration: 6490, loss = 2.2679343223571777\n",
      "iteration: 6500, loss = 2.259000539779663\n",
      "iteration: 6510, loss = 2.261866569519043\n",
      "iteration: 6520, loss = 2.2636501789093018\n",
      "iteration: 6530, loss = 2.2591166496276855\n",
      "iteration: 6540, loss = 2.2605907917022705\n",
      "iteration: 6550, loss = 2.267695903778076\n",
      "iteration: 6560, loss = 2.257070302963257\n",
      "iteration: 6570, loss = 2.272045373916626\n",
      "iteration: 6580, loss = 2.2602667808532715\n",
      "iteration: 6590, loss = 2.2657926082611084\n",
      "iteration: 6600, loss = 2.266000270843506\n",
      "iteration: 6610, loss = 2.2601866722106934\n",
      "iteration: 6620, loss = 2.2665653228759766\n",
      "iteration: 6630, loss = 2.261038303375244\n",
      "iteration: 6640, loss = 2.253657341003418\n",
      "iteration: 6650, loss = 2.256396770477295\n",
      "iteration: 6660, loss = 2.2511861324310303\n",
      "iteration: 6670, loss = 2.263845920562744\n",
      "iteration: 6680, loss = 2.2670071125030518\n",
      "iteration: 6690, loss = 2.2637057304382324\n",
      "iteration: 6700, loss = 2.2580947875976562\n",
      "iteration: 6710, loss = 2.2564191818237305\n",
      "iteration: 6720, loss = 2.259981155395508\n",
      "iteration: 6730, loss = 2.2641894817352295\n",
      "iteration: 6740, loss = 2.271418571472168\n",
      "iteration: 6750, loss = 2.254427194595337\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[454], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m     21\u001b[0m     p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# update \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# p.data += -lrs[i] * p.grad\u001b[39;00m\n",
      "File \u001b[0;32m~/dev/bpt/.venv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/bpt/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/bpt/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "iterations = 50000\n",
    "batch_size = 32000\n",
    "start = time.time()\n",
    "\n",
    "lre = torch.linspace(-3, 0 ,iterations)\n",
    "lrs = 10**lre\n",
    "\n",
    "for i in range(iterations):\n",
    "    mini_batch_indexes = torch.randint(0, X_train.shape[0], (batch_size,))\n",
    "    # forward pass\n",
    "    emb = C[X_train[mini_batch_indexes]]\n",
    "    h = torch.tanh(emb.view(-1, block_size*embedding_size) @ W1 + b1)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y_train[mini_batch_indexes])\n",
    "    if i % 10 == 0:\n",
    "        print(f'iteration: {i}, loss = {loss}')\n",
    "        \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update \n",
    "    for p in parameters:\n",
    "        # p.data += -lrs[i] * p.grad\n",
    "        p.data += -learning_rate * p.grad\n",
    "        \n",
    "    # track stats\n",
    "    lru.append(lre[i])\n",
    "    losses.append(loss.log10().item())\n",
    "        \n",
    "end = time.time()\n",
    "length = end - start\n",
    "print(f'Took {length:.{2}} seconds')\n",
    "\n",
    "# Calculate training set loss\n",
    "emb = C[X_train]\n",
    "h = torch.tanh(emb.view(-1, block_size*(embedding_size)) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, y_train)\n",
    "print(f'Full batch lost {loss=}')\n",
    "plt.plot(range(len(losses)),losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full batch lost loss=tensor(2.2762, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Calculate dev set loss\n",
    "emb = C[X_dev]\n",
    "h = torch.tanh(emb.view(-1, block_size*embedding_size) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, y_dev)\n",
    "print(f'Full batch lost {loss=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([27, 10])"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Only works if embedding size = 2\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carperfato.\n",
      "hari.\n",
      "kimle.\n",
      "reh.\n",
      "cassanden.\n",
      "jazonen.\n",
      "delyah.\n",
      "jarqai.\n",
      "nerania.\n",
      "chaiivin.\n",
      "leig.\n",
      "dham.\n",
      "join.\n",
      "quinn.\n",
      "shoilea.\n",
      "jadbiuo.\n",
      "jero.\n",
      "dearyximah.\n",
      "eunirraton.\n",
      "deci.\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)\n",
    "      h = torch.tanh(emb.view(1, -1) @ W1 + b1)\n",
    "      logits = h @ W2 + b2\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
